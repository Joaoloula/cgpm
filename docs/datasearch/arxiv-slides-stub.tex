\documentclass{article}

\usepackage{geometry}
  \geometry{
    a4paper,
    total={170mm,257mm},
    left=30mm,
    right=30mm,
    top=20mm,
}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage[labelfont=bf]{caption}
\usepackage[subrefformat=parens]{subcaption}
\usepackage[usenames,dvipsnames,svgnames]{xcolor}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{bbm}
\usepackage{chngcntr}
\usepackage{mdframed}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{placeins}
\usepackage{soul}
\usepackage{enumerate}

% fonts
\renewcommand{\rmdefault}{ptm}
\renewcommand{\sfdefault}{phv}

\newcommand*\Let[2]{\State {#1} $\gets$ {#2}}
\newcommand*\Sample[2]{\State {#1} $\sim$ {#2}}
\algnewcommand{\LineComment}[1]{\State \(\triangleright\) #1}

\newcommand{\balpha}{\bm\bm{\alpha}}
\newcommand{\bbeta}{\bm\beta}
\newcommand{\bgamma}{\bm\gamma}
\newcommand{\btheta}{\bm\bm{\Theta}}
\newcommand{\bTheta}{\bm\Theta}
\newcommand{\blambda}{\bm\lambda}
\newcommand{\G}{\mathcal{G}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{T}
\newcommand{\s}{\bm{s}}
\newcommand{\x}{\bm{x}}
\newcommand{\X}{\bm{X}}
\newcommand{\y}{\bm{y}}
\newcommand{\Y}{\bm{Y}}
\newcommand{\z}{\bm{z}}
\newcommand{\Z}{\bm{Z}}

\newcommand{\gpm}{\mathcal{G}}
\newcommand{\point}{\mathbf{x}}
\newcommand{\memberindex}{r}
\newcommand{\member}{\mathbf{x}_\memberindex}
\newcommand{\memberj}{x_{[\memberindex, j]}}
\newcommand{\numstates}{N_s}
\newcommand{\numtransitions}{N_t}
\newcommand{\data}{\mathcal{D}}
\newcommand{\datastar}{\data^\star}
\newcommand{\columns}{J}
\newcommand{\givens}{\mathcal{D}_g}
\newcommand{\comparison}{\mathcal{D}_c}
\newcommand{\comparisonindexes}{\mathcal{I}}
\newcommand{\comparisonmember}{\mathbf{x}_i}
\newcommand{\comparisonset}{\set{\x_{[r_i,Q]}}_{i \in \comparisonindexes}}
\newcommand{\score}{\textsc{score}}
\newcommand{\logscore}{\textsc{log score}}
\newcommand{\logpquery}{Q=\set{x_{[r,q_j]}}}
\newcommand{\values}{x_{[i,j]}}
\newcommand{\givenrow}{E=\set{x_[r,e_j]}}

\newcommand{\pop}{\mathcal{P}}
\newcommand{\mV}{\mathcal{V}}
\newcommand{\Vt}{\mathcal{V}_t}
\newcommand{\bx}{\bm{x}}
\newcommand{\bX}{\bm{X}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\mG}{\mathcal{G}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\bs}{\bm{s}}
\newcommand{\by}{\bm{y}}
\newcommand{\mI}{\mathcal{I}}
\newcommand{\mO}{\mathcal{O}}
\newcommand{\out}{\mathit{out}}
\newcommand{\bZ}{\bm{Z}}
\newcommand{\bz}{\bm{z}}
\newcommand{\bY}{\bm{Y}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\pg}{p_{\gpm}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\bxr}{\bx_r}
\newcommand{\bxstar}{\bx^\star}
\newcommand{\Dstar}{\text{D}^\star_n}
\newcommand{\schema}{\mathcal{S}}
\newcommand{\iV}{\mathit{V}}

\newcommand{\variable}[1]{X_{(#1)}}
\newcommand{\measurement}[1]{x_{(#1)}}
\newcommand{\latent}[1]{z_{(#1)}}
\newcommand{\query}[1]{q_{(#1)}}
\newcommand{\bxrn}[1]{\bx_{r_#1}^\star}
\newcommand{\set}[1]{\{{#1}\}}
\newcommand{\bcaption}[2]{\caption{\textbf{#1} {#2}}}

\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\logsumexp}{logsumexp}

\def\indep{\perp\!\!\!\perp}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}

\frenchspacing
\setlength{\floatsep}{5pt plus 1.0pt minus 2.0pt}
\setlength{\textfloatsep}{10pt plus 1.0pt minus 2.0pt}
\setlength{\abovecaptionskip}{2pt plus 0.0pt minus 1.0pt}
% \belowcaptionskip.
\setlength{\abovedisplayskip}{5pt plus 1.0pt minus 2.0pt}
\setlength{\belowdisplayskip}{5pt plus 1.0pt minus 2.0pt}

\newcommand{\algorithmbreak}{%
\vspace{5 pt}%
\noindent\rule{\linewidth}{0.4pt}%
\setcounter{ALG@line}{0}%
}

\begin{document}
\title{Data Search for the DP Mixture}
\author{Leo Casarsa}
\maketitle

\section{Pseudocode for CrossCat Logpdf}

Shortened version of data-type specific logpdf's.

\begin{algorithm}[H]
  \renewcommand{\thealgorithm}{}
  \caption{\texttt{logpdf} for CrossCat}
  \begin{algorithmic}[1]
    \footnotesize
    \Function{LogPdf}{
      \texttt{GPM}: $\mG$,
      \texttt{rowid}: $r$,
      \texttt{query}: $Q := \set{x_{r,q}}_{q=q_1}^{q_n}$,
      \texttt{evidence}: $E := \set{x_{r,e}}_{e=e_1}^{e_m}$}
      \For{$\iV \in \mathcal{V}$}
        \Comment{for each view $\iV$ in the variable partition $\mathcal{V}$}
        \Let{$\mathbf{w}$}{$\textsc{CRPWeights}(\mG, \iV, E)$}
          \Comment{retrieve posterior probabilities of proposed clusters given evidence}
        \Let{$K$}{$|\mathbf{w}|$} 
          \Comment{compute number of proposed clusters}
        \For{$k=1,\ldots,K$}
          \Comment{for each proposed cluster}
          \Let{$\iV_Q$}{$\set{q \in \iV : x_{r,q} \in Q}$}
            \Comment{retrieve the set of variables in the view $\iV$, 
              for which there is a query query}
          \For{$q \in \iV_Q$}
            \Comment{for each query query $x_{r,q}$ whose variable 
                     is in $\iV$}
            \Let{$\phi_{k,q}$}{$\textsc{GetSufficientStatistics}(\mG, q, k)$}
              \Comment{retrieve sufficient statistics of variable $q$ in cluster $k$}
            \If{$\texttt{data-type}(q) = \texttt{binary}$}
              \Comment{for binary variables}
              \Let{$p_{r,q|k}$}{$
                \textsc{BetaBernoulli}(x_{r,q};\phi_{k, q})$
              } \Comment{compute Beta-Bernoulli density}
              \EndIf

              \If{$\texttt{data-type}(q) = \texttt{nominal}$}
                \Comment{for nominal variables}
                \Let{$p_{r,q|k}$}{$
                  \textsc{DirichletCategorical}(x_{r,q};\phi_{k, q})$
                } \Comment{compute Dirichlet-Categorical}
              \EndIf

              \If{$\texttt{data-type}(q) = \texttt{numerical}$}
                \Comment{for numerical variables}
                \Let{$p_{r,q|k}$}{$
                  \textsc{NIGNormal}(x_{r,q};\phi_{k, q})$
                } \Comment{compute Normal-Inverse-Gamma-Normal density}
              \EndIf

          \EndFor
        \EndFor
        \vspace{5pt}
        \Let{$p_\iV$}{$\displaystyle \sum_{k=1}^K 
              \left(\prod_{q \in \iV_Q} p_{r,q|k}\right) w_k$}
          \Comment{compute density for query variable in $\iV$}
      \EndFor
      \State \Return{$\displaystyle \sum_{\iV \in \mathcal{V}}\log(p_\iV)$}
        \Comment{return overall log density of query}
    \EndFunction

  \algorithmbreak
  \Function{CRPWeights}{
    \texttt{GPM}: $\mG$,
    \texttt{view}: $\iV$,
    \texttt{evidence}: $E$}
  \EndFunction

  \algorithmbreak
  \Function{GetSufficientStatistics}{
    \texttt{GPM}: $\mG$,
    \texttt{dimension}: $d$,
    \texttt{cluster}: $k$}
  \EndFunction

  \algorithmbreak
  \Function{BetaBernoulli}{
    \texttt{query}: $x$,
    \texttt{parameters}: $\vec{\lambda} := \set{\beta_1', \beta_0'}$
    }
    \Let{$m$}{$\frac{\beta_1'}{\beta_0' + \beta_1'}$}
      \Comment{compute parameter for collapsed Beta-Bernoulli}
    \Let{$p_x$}{$m^x \, (1-m)^{1-x}$}
      \Comment{compute density of Bernoulli with collapsed parameter}
    \State \Return{$p_x$} \Comment{return Beta-Bernoulli density}
  \EndFunction

  \algorithmbreak
  \Function{DirichletCategorical}{
    \texttt{query}: $x$,
    \texttt{parameters}: $\vec{\lambda} := \set{\beta_i'}_{i=1}^{c}$
  }
  \Let{$p_x$}{$\frac{\beta_{x}'}{\sum_{i=1}^c \beta_i'}$} 
    \Comment{compute density of Categorical with collapsed parameters}
  \State \Return{$p_x$} \Comment{return Dirichlet-Categorical density}
  \EndFunction

  \algorithmbreak
  \Function{NIGNormal}{
    \texttt{query}: $x$,
    \texttt{parameters}: $\vec{\lambda} := \set{m', V', a', b'}$
  }
  \LineComment{The formula I found from Murphy follows an unfamiliar parameterization of the student-t}
  \EndFunction

  \end{algorithmic}
\end{algorithm}

\pagebreak
\section{PseudoCode for CrossCat Incorporate} 
-- Inline math for incorporate

\begin{algorithm}[H]
  \renewcommand{\thealgorithm}{}
  \caption{\texttt{incorporate} for CrossCat}
  \begin{algorithmic}[1]
    \footnotesize
    \Function{Incorporate}{
      \texttt{GPM}: $\mG$,
      \texttt{rowid}: $r$,
      \texttt{query}: $(Q_x,Q_z):=(\set{\x_{r,q}}_{q=q_1}^{q_n}, \set{\z_{r,\iV}}_{\iV=\iV_1}^{\iV_m})$}
    \Let{$\mG'$}{$\mG$} \Comment{initialize output GPM with input GPM $\mG$}
    \Let{$\mD_{\mG'}$}{$\mD_\mG \cup (r,Q_x)$}
      \Comment{append the observable query values to dataset $\mD$}
    \For{$\iV \in \mathcal{V}$}
      \Comment{for each view in the variable partition}
      \If{$z_{r,\iV} \in Q_z$}
        \Comment{if user assigned a mixture component to 
                 the view}
        \Let{$k$}{$z_{r,\iV}$}
      \Else
        \Comment{if user did not assign a mixture component
        for the query} 
        \Let{$k$}{\textsc{SimulateLatent}
                  $(\mG, r, \iV, Q_x)$}
          \Comment{sample component for view 
                   $\iV$ conditioned on $Q_x$}
      \EndIf
      \Let{$\Z_{\mG'}$}{$\Z_\mG \cup (r, \iV, k)$}
        \Comment{include component assignment into
                 row-specific latents}
      \For{$ \set{d \in \iV: x_{r,d} \in Q_x}$}
        \Comment{for each variable in view $\iV$
                 for which there is a query observation}
        \Let{$\phi'_{(k, d)}$}{$\textsc{ComputeUpdatedParameters}(\mG', k, d)$}
          \Comment{compute parameters for $d$ after observing $Q_x$}
        \Let{$\bm{\Theta}_{\mG'}$}{$\set{\bm{\Theta}_\mG \setminus 
            \phi_{k, d}} \cup \phi'_{k, d}$}
          \Comment{update the population-level latents}
       \EndFor
    \EndFor
    \State \Return{$\mG'$} \Comment{return GPM with incorporated queries}
    \EndFunction
    
    \algorithmbreak
    \Function{SimulateLatent}{}
    \State{ Should I write this?}
    \EndFunction

    \algorithmbreak
    \Function{ComputeUpdatedParameters}{}
    \State {Not sure yet if I should Include this}
    \EndFunction

    \algorithmbreak
    \Function{Unincorporate}{
      \textsc{GPM}: $\mG$,
      \textsc{rowid}: $r$}
    \Let{$\mG'$}{$\mG$} \Comment{initialize output GPM with input GPM $\mG$}
    \Let{$Q_x$}{$\set{x_{r,d}}_{d=1}^{D}$}
      \Comment{retrieve row values from dataset}
    \Let{$\mD_{\mG'}$}{$\mD_\mG \setminus (r, Q_x)$}
      \Comment{remove row $r$ from the dataset}
      \For{$\iV \in \mathcal{V}$}
        \Comment{for each view in the variable partition}
        \Let{$k$}{$z_{r,\iV}$} 
          \Comment{retrieve view component assignment}
        \Let{$\Z_{\mG'}$}{$\Z_\mG \setminus (r, V, k)$}
          \Comment{remove view component assignment from row-specific latents}
        \For{$d \in \iV$}
          \Comment{for each dimension/variable in view $\iV$}
          \Let{$\phi'_{k, d}$}{$\textsc{ComputeRevertedParameters}(\mG', k, d)$}
            \Comment{compute parameters for $d$ after removing $r$ from dataset}
          \Let{$\bm{\Theta}_{\mG'}$}{$\set{\bm{\Theta}_\mG \setminus 
              \phi_{k, d}} \cup \phi'_{k, d}$}
            \Comment{update the population-level latents}
        \EndFor
      \EndFor
    \State \Return{$\mG'$} \Comment{return GPM with unincorporated row}
    \EndFunction

    \algorithmbreak
    \Function{ComputeRevertedParameters}{}
    \State {Should I write this?}
    \EndFunction

  \end{algorithmic}
\end{algorithm}

\section{PseudoCode for CrossCat Logpdf-Set}
-- Make context as an argument

\begin{algorithm}[H]
  \renewcommand{\thealgorithm}{}
  \caption{\texttt{logpdf-set} for CrossCat}
  \begin{algorithmic}[1]
    \Function{LogPdf-Set}{
      \texttt{GPM}: $\mG$,
      \texttt{query}: $Q := \set{(r_q, \bm{x_q}, z_q)}_{q=q_1}^{q_n}$,
      \texttt{evidence}: $E := \set{(r_e, \bm{x_e}, z_e)}_{e=e_1}^{e_m}$,
      \texttt{context-variable}: $v$}
    \footnotesize
    \Let{$T$}{$\set{(r, \x_r, \z_r): r \in \mD_\mG \cap (Q \cup E)}$}
      \Comment {store values and category of query and evidence rows already in dataset}
    \For {$r \in T$} \Comment{for query and evidence rows already in the dataset}
      \Let{$\mG'$}{\textsc{Unincorporate}$(\mG, r)$}
        \Comment{unincorporate rows from the GPM before evaluating the logpdf}
    \EndFor

    \Let{$\log p_j$}{\textsc{Joint-LogPdf-Set}$(\mG', Q \cup E, v, 0)$} 
      \Comment{compute the joint logpdf of query and evidence in the context of $v$}

    \Let{$\log p_m$}{\textsc{Joint-LogPdf-Set}$(\mG', E, v, 0)$}
      \Comment{compute the marginal logpdf of evidence in the context of $v$}

    \State \Return{$\log p_j - \log p_m$}
      \Comment{return $\log p_\mG(Q|E,C) = \log p_\mG(Q,E|C) - \log p_\mG(E|C_e)$}
    \EndFunction

    \vspace{5 pt}
    \noindent\rule{\linewidth}{0.4pt}
    \setcounter{ALG@line}{0}
    {\normalsize \Function{Joint-LogPdf-Set}{
      \texttt{GPM}: $\mG$,
      \texttt{query}: $Q = \set{(r_q, \bm{x_q}, z_q)}_{q=1}^{q_n}$,
      \texttt{context-variable}: $v$,
      \texttt{counter}: $i$}}

    \Let{$Q_v$}{$\set{(r, x, z): (r, x, z) \in Q \cap \iV_v}$}    
      \Comment{retrieve the query variables restricted to the view to which $v$ belongs}
    \Let{$\log p_{Q_v}$}{$-\infty$} 
      \Comment{initialize output log probability as $\log p_\mG(Q) = \log 0$}

    \If{$i=|Q|$} \Comment{base case, reached after factoring out all rows in $Q$}
      \Let{$\log p_{Q_v}$}{$0$} \Comment{base case output, $\log p_\mG(Q)=0$ when $Q=\emptyset$}

    \vspace{5 pt}
    \Else \Comment{recursive case, factorize density using chain rule}
      \If{$z_{q_i}^v \in Q_v$} 
        \Comment{if user specified a component for row $r_{q_i}$}
        \Let{$K$}{$\set{z_{q_i}^v}$}
          \Comment{only possible component is the assigned component}

      \Else \Comment{if user did not specify a component for row $r_{q_i}$}
        \Let{$n_K$}{$|\set{\Z_\mG}|+1$}
          \Comment{get number of possible components}
        \Let{$K$}{$\set{1,\ldots, n_k}$}
          \Comment{store possible component assignments for row $r_{q_i}$}
      \EndIf

      \For {$k \in K$} \Comment{for each possible component}
        \Let{$z_{q_i}^v$}{$k$} \Comment{component $k$ will be assigned to row}
        \Let{$\log p_i^v$}{$\textsc{LogPdf}
            (\mG, r_{q_i}, (x_{q_i}^v, z_{q_i}^v), \emptyset)$}
            \Comment{evaluate joint density for row $r_{q_i}$}
        \Let{$\mG'$}{\textsc{Incorporate}($\mG, r_{q_i}, (x_{q_i}^v, z_{q_i}^v)$)}
            \Comment{incorporate row and component assignment}
      \Let{$\log p_i$}{$\log p_i + \textsc{Joint-LogPdf-Set}(\mG', Q, v, i+1)$}
        \Comment{factor out row $r_{q_i}$ from the joint density of query}
      \Let{$\log p_{Q_v}$}{$ \log \left( p_{Q_v} + p_i \right)$} 
        \Comment{marginalize out the category assignments}
      \EndFor
    \EndIf
    \State \Return{$\log p_{Q_v}$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}



\section{PseudoCode for Joint Logpdf-Set}
-- Remove helper

\section{PseudoCode for Relevance Search}
-- Add missing Unincorporate 

\begin{algorithm}[H]
  \renewcommand{\thealgorithm}{}
  \caption{Relevance Search for CrossCat}
  \begin{algorithmic}[1]
  \footnotesize
  \Function{Relevance-Score}{
    \texttt{GPM}: $\mG$,
    \texttt{query}: $Q := (r_q, \bm{x_q})$,
    \texttt{evidence}: $E := \set{(r_e, \bm{x_e})}_{e=e_1}^{e_m}$,
    \texttt{context-variable}: $v$}
  \Let{$\log p_{H_1}$}{$0$} \Comment{initialize the hypotheses log-probabilities}
  \Let{$\log p_{H_2}$}{$0$}
  \For{$r \in Q \cup E$} \Comment{unincorporate each row before search}
    \Let{$\mG'$}{$\textsc{Unincorporate}(\mG, r)$}
  \EndFor
  \Let{$n_K$}{$|\set{\Z_\mG^v}|+1$} 
    \Comment{compute number of proposed components in the view of  $v$}
  \For{$k_q \in \set{1,\ldots,n_K}$} 
    \Comment{for each proposed component}
    \Let{$\z_q$}{$(r_q,k_q)$} 
      \Comment{name current component assignments for the query row}

    \If{$k_q = n_K$} \Comment{if query has been assigned an empty component}
      \Let{$n_K'$}{$n_K+1$} \Comment{evidence may be assigned another empty component}
    \Else \Comment{otherwise}
      \Let{$n_K'$}{$n_K$} \Comment{no need for another empty component}
    \EndIf

    \For{$k_e \in \set{1,\ldots,n_K'} $} 
      \Comment{for each possible component assignment for the evidence set}
      \Let{$z_e$}{$\set{(r_e,k_e)}$} 
        \Comment{name current component assignment for the evidence set}
        
        \Let{$J$}{$Q \cup E \cup \set{z_q} \cup \set{z_e}$}
          \Comment{retrieve joint set of query and density with respective components}
        \Let{$\log p_J$}{$\textsc{LogPdf-Set} (
          \mG, J, v)$}
          \Comment{evaluate joint density}
    
      \If{$k_q = k_t$} 
        \Comment{$H_1$: query and evidence are in the same component}
        \Let{$\log p_{H_1}$}{$\log \left( p_{H_1} + p_J\right)$ }
          \Comment {marginalize over previous component assignments}

      \vspace{5 pt}
      \Else 
        \Comment{$H_2$: query and target are in different categories}
        \Let{$\log p_{H_2}$}{$\log \left( p_{H_2} + p_J\right)$ }
          \Comment {marginalize over previous component assignments}
      \EndIf
    \EndFor
  \EndFor 
  \State \Return {$\log p_{H_1} - \log p_{H_2}$} \Comment{return the log of the relevance score}
  \EndFunction

  \vspace{5 pt}
  \noindent\rule{\linewidth}{0.4pt}
  \setcounter{ALG@line}{0}
  \Function{Relevance-Search}{
    \texttt{GPM}: $\mG$,
    \texttt{evidence}: $E := \set{(r_e, \bm{x_e})}_{e=e_1}^{e_m}$,
    \texttt{context-variable}: $v$}
  \For{$R \in \mD$} \Comment{for each row in the dataset}
    \Let{$\mathcal{S}[r]$}{$\textsc{Relevance-Score}(\mG, R, E, v)$}
      \Comment{compute and store the score for each row in the dataset}
  \EndFor
  \State \Return{$\textsc{Sorted}(\mathcal{S})$}
    \Comment{return the sorted score for each row} 
  \EndFunction
  \end{algorithmic}
\end{algorithm}

\section{Math for CrossCat generative model}
-- Include the models for binary, categorical and numerical.

\begin{align*}
  \alpha_D &\sim \text{Gamma}(k=1, \theta=1)  & \\
  \vec{\lambda_d} &\sim V_d & \text{for each } d \in [D]  \\
  \mathcal{V}_D &\sim \text{CRP}( \alpha_D, [D]) & \text{partition variables into views}\\
  \alpha_\iV &\sim \text{Gamma}(k=1, \theta=1)  & \text{for each view } \iV \in \mathcal{V}_D \\
  \mathcal{C}_\iV &\sim \text{CRP}( \alpha_\iV, [R]) &\text{partition rows into components for each } \iV \in \mathcal{V}_D\\
  x_{r,d} &\sim {\displaystyle \mathit{ML}_d( \vec{\lambda_d}, \mathcal{C}_{\iV,d})} 
                                        & \text{for each } d \in [D]
\end{align*}

\section{Math for CrossCat joint density}
--
\begin{align*}
  P&(\alpha_D, \mathcal{V}_D, \set{\vec{\lambda_d}}, \set{\alpha_\iV}, \set{\mathcal{C}_\iV}, x_{r,d})
    \\
   &= P(\alpha_D)  \,   
      P(\mathcal{V}_D|\alpha_D, [D])   
      \prod_{d \in [D]} P(\vec{\lambda_d}) \, 
      P(\set{\alpha_\iV}) \,
      P(\mathcal{C}_{\iV,d}|\alpha_\iV) \,
      P(x_{r,d}| \lambda_d, \mathcal{C}_{\iV,d})
  \\
    &= e^{-\alpha_D}\, 
       \text{CRP}(\mathcal{V}_D; \alpha_D, [D])
       \,\prod_{d \in [D]} V_d(\vec{\lambda_d}) \,
       e^{-\alpha_\iV}\,
       \mathit{ML}_d(x_{(r,d)}; \lambda_d, \mathcal{C}_{\iV, d})\\
\end{align*}

\subsection{Marginal Likelihood}

\subsubsection{$d$ is \texttt{binary}}

\begin{align*}
  \lambda_d &:= \set{\beta_i} 
              \sim \set{\text{Gamma(1,1)}}_i & 
              \text{for } i=0,1
\\
  \beta'_{i,k} &= \beta_i + 
                 |\set{r: z_r=k, x_{r,d}=i}| &
                 \text{for } i=0,1 
\\
  ML_d&(x_{(r,d)}; \lambda_d, \mathcal{C}_{\iV,d}) 
        = \text{Bernoulli}\left(
        \frac{\beta'_{1,k}}{\beta'_{1,k}+\beta_{0,k}'}
        \right) &  
\end{align*}

\subsubsection{$d$ is \texttt{nominal}}

\begin{align*}
  \lambda_d &:= \set{\beta_i} 
              \sim \set{\text{Gamma(1,1)}}_i & 
              \text{for } i=1,\ldots, c
\\
  \beta'_{i,k} &= \beta_i + 
                 |\set{r: z_r=k, x_{r,d}=i}| &
                 \text{for } i=1,\ldots, c
\\
  ML_d&(x_{(r,d)}; \lambda_d, \mathcal{C}_{\iV,d}) 
        = \text{Categorical}\left(
        \frac{\beta'_{1,k}}{\sum_{i=1}^c \beta'_{i,k}}
        \right) &
\end{align*}

\subsubsection{$d$ is \texttt{numerical}}

\begin{align*}
  \lambda_d &= (m, V, a, b) & 
\\
  n_k &:= |\set{r \in [R]: z_r = k}| 
\\
  \bar{x}_{d,k} &:= \frac{1}{n_k}\sum_{r: z_r = k}x_{(r, d)} & 
\\
  V_k^{-1} &:= V^{-1} + n_k & 
\\
  m_k &:= V'_k(V^{-1}m) + n_k\bar{x}_{d,k} &
\\
  a_k &:= a + \frac{n_k}{2} & 
\\
  b_k &:= b + \frac{1}{2} 
         \left[m^2 V^{-1} + \sum_{r: z_r = k} x^{2}_{(r, d)} -m_k^2 V_k^{-1} \right]& 
\\
  ML_d&(x_{(r,d)}; \lambda_d, \mathcal{V}_R^{\iV_d}) = 
        \text{Student-t}_{2a_k}
        \left(m_k, \frac{b_k(1+V'_k)}{a_k}\right) &
\end{align*}

\section{Math for relevance score}

We formulate the relevance score $\mathcal{S}$ of a query row
$Q:=(r_q, \bm{x}_q)$ with respect to an evidence set of rows
$E:=\set{r_{e}, \bm{x}_e}_{e=1}^{e_m}$ as the log-posterior odds of
the two following hypotheses:

\begin{enumerate}
\item $H_1 = $ ``all rows in query and evidence belong to the same, unknown, component.''
\item $H_2 = $ ``all rows in evidence belong to the same, unknown, component, which is different from the component of the query row.''
\end{enumerate}

\begin{align*}
  \mathcal{S}(Q;E) &:= \frac{P(H_1|Q,E)}{P(H_2|Q,E)} = \frac{P(H_1,Q,E)}{P(H_2,Q,E)} \\
      &= \frac{\sum_k P(z_Q=z_E=k,Q,E)}{\sum_{k_q \neq k_e} P(z_Q=k_q, z_E=k_e,Q,E)}
\end{align*}

where $z_Q, z_E$ are the component assignments for the rows in query and evidence, respectively. The relevance score is computed straightforwardly through the \textsc{LogPdf-Multirow} interface: 



\section{Math for recovering Bayes Sets}
--
The Bayes Sets score $\mathcal{S}_\iV$ of a query row
$Q:=(r_q, \bm{x}_q)$ with respect to an evidence set of rows
$E:=\set{r_{e}, \bm{x}_e}_{e=1}^{e_m}$ is formulated as:


\begin{align*}
  \mathcal{S}_\iV(Q;E) &:= \frac{P_0(Q,E)}{P_0(Q)P_0(E)}
\end{align*}

where $P_0$ is the prior predictive distribution,

\begin{align*}
  P_0(x) &= \int p(x|\theta) \, p(\theta) \; d\theta \\
  P_0(Q) &= \int \prod_{x_i \in Q} p(x_i|\theta)\, p(\theta)\; d\theta \\
  P_0(E) &= \int \prod_{x_i \in E} p(x_i|\theta)\, p(\theta)\; d\theta \\
  P_0(Q, E) &= \int \prod_{x_i \in Q \cup E} p(x_i|\theta)\, p(\theta)\; d\theta 
\end{align*}

where the probabilistic model $p(x|\theta)$ and the prior $p(\theta)$ 
have been specified by the user.

We will show that the relevance score is equivalent up to constant
factor to the Bayes sets score if the GPM has no incorporated
queries.

If the GPM has no incorporated queries, there are only
two possible component assignments for the query and evidence
sets,

\begin{align*}
  \mathcal{S}(Q;E) &= \frac{\sum_k P_\mG(z_Q=z_E=k,Q,E)}
                     {\sum_{k_q \neq k_e} P_\mG(z_Q=k_q, z_E=k_e,Q,E)}
\\
                   &= \frac{P_\mG(z_Q=z_E=0,Q,E)} 
                      {P_\mG(z_Q=0, z_E=1,Q,E)}
\\
                   &=\frac{P_\mG(Q,E|z_Q=z_E=0)}
                     {P_\mG(Q,E |z_Q=1, z_E=0)}
                     \cdot
                     \frac{P_\mG(z_Q=z_E=0)}
                     {P_\mG(z_Q=0, z_E=1)} 
\\
                   &\propto \frac{P_{0}(Q,E)}
                     {P_{0}(Q)P_{0}(E)}
\end{align*}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
